{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CIFAR-10-CLASSIFCATION MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpickle(file): # - http://www.cs.toronto.edu/~kriz/cifar.html - this will open the file and return a dictionary \n",
    "    import pickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = unpickle(\"cifar-10-batches-py\\data_batch_1\") # loading data_batch_1 into the variable dataset\n",
    "metadata = unpickle(r\"cifar-10-batches-py\\batches.meta\") # These are the label names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of importing the dataset directly, I decided to install it as a zip from online, to practice loading and wrangling the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The website source of the dataset states:\n",
    "\n",
    " **data** -- a 10000x3072 numpy array of uint8s. Each row of the array stores a 32x32 colour image. The first 1024 entries contain the red channel values, the next 1024 the green, and the final 1024 the blue. The image is stored in row-major order, so that the first 32 entries of the array are the red channel values of the first row of the image.\n",
    "\n",
    "**labels** -- a list of 10000 numbers in the range 0-9. The number at index i indicates the label of the ith image in the array data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys([b'batch_label', b'labels', b'data', b'filenames']) dict_keys([b'num_cases_per_batch', b'label_names', b'num_vis'])\n"
     ]
    }
   ],
   "source": [
    "print(dataset.keys(),\n",
    "metadata.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the the relevant items in the dataset and metadata dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = dataset[b'labels']\n",
    "data = dataset[b'data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)\n",
    "len(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, there are 10000 images + labels in our dataset. I will split the dataset in 80-20 training-testing into the following variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = data[:8000]\n",
    "X_test = data[8000:10000]\n",
    "\n",
    "y_train = labels[:8000]\n",
    "y_test = labels[8000:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([b'num_cases_per_batch', b'label_names', b'num_vis'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[b'airplane',\n",
       " b'automobile',\n",
       " b'bird',\n",
       " b'cat',\n",
       " b'deer',\n",
       " b'dog',\n",
       " b'frog',\n",
       " b'horse',\n",
       " b'ship',\n",
       " b'truck']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keys = metadata[b'label_names']\n",
    "keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a label of 0 should be a airplane, a label of 1 should be of a automobile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0]\n",
    "# plt.imshow(X_train[0]) # This line throws a TypeError: Invalid shape (3072,) for image data. I realized, this is actually a 32 x 32 size image with RGB, however since there is nothing delimiting the rows\n",
    "# So it just shows as 3072 - The images seem to have flattened, meaning I would need to \"unflattern\" them if I want to view them\n",
    "# https://stackoverflow.com/questions/36967920/numpy-flatten-rgb-image-array \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x27b034e4d30>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "reshaped_image = test.reshape(32,32,3) # 32 x 32 (image size) with the 3 denoting the RGB values\n",
    "\n",
    "plt.imshow(reshaped_image)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm going to have to reformat the array If I want to view the image, I will create a function that will allow me to view the image without changing the data structure of the original image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image(index = 0, feature_data = X_train, label_data = y_train, keys = keys):\n",
    "        \"\"\"\n",
    "        input feature, and label data, and the keys dictionary, optional param index\n",
    "\n",
    "        displays the image + label\n",
    "        \"\"\"\n",
    "        # The images are stored as a 3072 element vector, I need to reshape this into a tensor \n",
    "        # The image source states \"The first 1024 bytes are the red channel values, the next 1024 the green,\n",
    "        # and the final 1024 the blue. The values are stored in row-major order, so the first 32 bytes are the red channel values of the first row of the image.\"\n",
    "        res = feature_data[index].reshape(3,32,32) \n",
    "        #matplotlib reads the RGB channels last in the matrix. So I'm rearranging the tensor accordingly\n",
    "        res = np.transpose(res, axes=[1, 2, 0]) \n",
    "\n",
    "        plt.imshow(res)\n",
    "        plt.title(keys[label_data[index]].decode(\"utf-8\")) # adds the label; the decode removes the b and '' from the label for a pleasant viewing experience\n",
    "        plt.axis('off') # I don't want the axis to show either\n",
    "        plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_image()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beautiful!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating model using keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = X_train / 255.0, X_test / 255.0\n",
    "# since the values range from 1 to 256, I want them to range from 0-1, i will divide by 255 to achieve this. \n",
    "# Sources state this allows the model to perform more efficiently\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_v1 = tf.keras.Sequential([ # My model should have one input and one output tensor\n",
    "    tf.keras.layers.Dense(128, activation=tf.nn.relu, input_shape = (3072,)), # input shape of 3072\n",
    "    tf.keras.layers.Dense(128, activation=tf.nn.relu), # relu is a activation function, which determines the\n",
    "    # output shape of each node in the layer\n",
    "    tf.keras.layers.Dense(10) ])\n",
    "\n",
    "\n",
    "    \n",
    "# There is 2 layers with 2 nodes each, and a output layer with 10 nodes, each representing one of the \n",
    "# 10 possible categories\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attempt 1\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this attempt, I decided to follow this [Walkthrough](https://www.tensorflow.org/tutorials/customization/custom_training_walkthrough) on the tensorflow website. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This walkthrough involved defining your own loss and gradient functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'loss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-82b3d751ec4f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0ml\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_v1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Loss test: {}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'loss' is not defined"
     ]
    }
   ],
   "source": [
    "l = loss(model_v1, X_train, y_train, training=False)\n",
    "print(\"Loss test: {}\".format(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad(model,inputs,targets):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss_value = loss(model, inputs, targets, training = True)\n",
    "    return loss_value, tape.gradient(loss_value, model.trainable_variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating optimizer \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0, Initial Loss: 2.464334011077881\n",
      "Step: 1,         Loss: 2.399326801300049\n"
     ]
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.SGD(learning_rate = 0.01)\n",
    "loss_value, grads = grad(model_v1, X_train, y_train)\n",
    "print(\"Step: {}, Initial Loss: {}\".format(optimizer.iterations.numpy(),\n",
    "                                          loss_value.numpy()))\n",
    "\n",
    "optimizer.apply_gradients(zip(grads, model_v1.trainable_variables))\n",
    "\n",
    "print(\"Step: {},         Loss: {}\".format(optimizer.iterations.numpy(),\n",
    "                                          loss(model_v1, X_train, y_train, training=True).numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_v1.compile(loss = l, optimizer = optimizer, metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Failed to find data adapter that can handle input: <class 'numpy.ndarray'>, (<class 'list'> containing values of types {\"<class 'int'>\"})",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-666022cd5440>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel_v1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\maste\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\maste\\anaconda3\\lib\\site-packages\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36mselect_data_adapter\u001b[1;34m(x, y)\u001b[0m\n\u001b[0;32m    983\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0madapter_cls\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    984\u001b[0m     \u001b[1;31m# TODO(scottzhu): This should be a less implementation-specific error.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 985\u001b[1;33m     raise ValueError(\n\u001b[0m\u001b[0;32m    986\u001b[0m         \u001b[1;34m\"Failed to find data adapter that can handle \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    987\u001b[0m         \"input: {}, {}\".format(\n",
      "\u001b[1;31mValueError\u001b[0m: Failed to find data adapter that can handle input: <class 'numpy.ndarray'>, (<class 'list'> containing values of types {\"<class 'int'>\"})"
     ]
    }
   ],
   "source": [
    "model_v1.fit(X_train,y_train,batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When trying to compile my model I run into this error\n",
    "\n",
    "    ValueError: Failed to find data adapter that can handle input: <class 'numpy.ndarray'>, (<class 'list'> containing values of types {\"<class 'int'>\"})\n",
    "\n",
    "I think I can fix this by converting my datasets into numpy arrays\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'UnreadVariable' shape=() dtype=int64, numpy=1>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_v2 = np.array(X_train)\n",
    "y_train_v2 = np.array(y_train)\n",
    "X_test_v2 = np.array(X_test)\n",
    "y_test_v2 = np.array(y_test)\n",
    "\n",
    "l = loss(model_v1, X_train_v2, y_train_v2, training=False)\n",
    "\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate = 0.01)\n",
    "loss_value, grads = grad(model_v1, X_train_v2, y_train_v2)\n",
    "optimizer.apply_gradients(zip(grads, model_v1.trainable_variables))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_v1.compile(loss = l, optimizer = optimizer, metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"c:\\Users\\maste\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1051, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\maste\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1040, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\maste\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1030, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\Users\\maste\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 890, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"c:\\Users\\maste\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 948, in compute_loss\n        return self.compiled_loss(\n    File \"c:\\Users\\maste\\anaconda3\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 184, in __call__\n        self.build(y_pred)\n    File \"c:\\Users\\maste\\anaconda3\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 133, in build\n        self._losses = tf.nest.map_structure(self._get_loss_object, self._losses)\n    File \"c:\\Users\\maste\\anaconda3\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 272, in _get_loss_object\n        loss = losses_mod.get(loss)\n    File \"c:\\Users\\maste\\anaconda3\\lib\\site-packages\\keras\\losses.py\", line 2372, in get\n        raise ValueError(\n\n    ValueError: Could not interpret loss function identifier: 2.362820863723755\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-35-8477651ca2b8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel_v1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_v2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train_v2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\maste\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\maste\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m                     \u001b[0mretval_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mld\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mld\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mld\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m                 \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"c:\\Users\\maste\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1051, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\maste\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1040, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\maste\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1030, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\Users\\maste\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 890, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"c:\\Users\\maste\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 948, in compute_loss\n        return self.compiled_loss(\n    File \"c:\\Users\\maste\\anaconda3\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 184, in __call__\n        self.build(y_pred)\n    File \"c:\\Users\\maste\\anaconda3\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 133, in build\n        self._losses = tf.nest.map_structure(self._get_loss_object, self._losses)\n    File \"c:\\Users\\maste\\anaconda3\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 272, in _get_loss_object\n        loss = losses_mod.get(loss)\n    File \"c:\\Users\\maste\\anaconda3\\lib\\site-packages\\keras\\losses.py\", line 2372, in get\n        raise ValueError(\n\n    ValueError: Could not interpret loss function identifier: 2.362820863723755\n"
     ]
    }
   ],
   "source": [
    "model_v1.fit(X_train_v2,y_train_v2,batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I seem to get this error:\n",
    "\n",
    "    ValueError: Could not interpret loss function identifier: 2.3097445964813232\n",
    "\n",
    "I believe this is a problem with my custom loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_v1.compile(loss = 'SparseCategoricalCrossentropy', optimizer = optimizer, metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "225/225 [==============================] - 2s 6ms/step - loss: 12.8475 - accuracy: 0.1032 - val_loss: 12.2801 - val_accuracy: 0.1175\n",
      "Epoch 2/5\n",
      "225/225 [==============================] - 1s 5ms/step - loss: 13.6690 - accuracy: 0.0985 - val_loss: 14.2625 - val_accuracy: 0.1150\n",
      "Epoch 3/5\n",
      "225/225 [==============================] - 1s 5ms/step - loss: 14.4630 - accuracy: 0.1022 - val_loss: 14.7279 - val_accuracy: 0.0862\n",
      "Epoch 4/5\n",
      "225/225 [==============================] - 1s 5ms/step - loss: 14.4682 - accuracy: 0.1024 - val_loss: 14.7279 - val_accuracy: 0.0862\n",
      "Epoch 5/5\n",
      "225/225 [==============================] - 1s 5ms/step - loss: 14.4682 - accuracy: 0.1024 - val_loss: 14.7279 - val_accuracy: 0.0862\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x221c1a9fbe0>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_v1.fit(X_train_v2,y_train_v2,batch_size=32,epochs = 5, validation_split = 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss and accuracy are not changing between epochs, There seems to be a problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 0s 3ms/step - loss: 14.5626 - accuracy: 0.0965\n"
     ]
    }
   ],
   "source": [
    "val_loss, val_acc = model_v1.evaluate(X_test_v2, y_test_v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 0s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "predictions = model_v1.predict(X_test_v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'deer'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = X_test_v2[0]\n",
    "show_image(0, X_test, y_test)\n",
    "keys[np.argmax(predictions[0])].decode(\"utf-8\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My model decided to predict this cat as a deer, it has a accuracy of 0.0990 and high high loss. My model guesses correctly 10 percent of the time, which should be expected with 10 outputs if you were to guess at random. My model doesn't work at all, it is completely broken. In hindsight I should not have tried to create my own loss function + optimiser, as I do not understand how it works. I will try again, to create a model using prebuilt loss and optimiser functions. I will focus on developing a accurate and working model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attempt 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since data is already flattened, in the shape (3072,), I will not need to flatten the data. I will creating a model with the provided optimiser and loss functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras #so I don't have to type tf. everytime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(128, activation = 'relu'), # 128 neurons\n",
    "    keras.layers.Dense(128, activation = 'relu'),\n",
    "    keras.layers.Dense(128, activation = 'relu'),\n",
    "    keras.layers.Dense(10, activation = 'softmax') # Since there are 10 output classifications\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "250/250 [==============================] - 2s 5ms/step - loss: 2.0888 - accuracy: 0.2271\n",
      "Epoch 2/30\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 1.9004 - accuracy: 0.3043\n",
      "Epoch 3/30\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 1.8157 - accuracy: 0.3478\n",
      "Epoch 4/30\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 1.7653 - accuracy: 0.3646\n",
      "Epoch 5/30\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 1.7057 - accuracy: 0.3836\n",
      "Epoch 6/30\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 1.6716 - accuracy: 0.3950\n",
      "Epoch 7/30\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 1.6429 - accuracy: 0.4025\n",
      "Epoch 8/30\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 1.6124 - accuracy: 0.4176\n",
      "Epoch 9/30\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 1.5676 - accuracy: 0.4375\n",
      "Epoch 10/30\n",
      "250/250 [==============================] - 2s 6ms/step - loss: 1.5492 - accuracy: 0.4392\n",
      "Epoch 11/30\n",
      "250/250 [==============================] - 2s 7ms/step - loss: 1.5168 - accuracy: 0.4475\n",
      "Epoch 12/30\n",
      "250/250 [==============================] - 2s 7ms/step - loss: 1.4801 - accuracy: 0.4652\n",
      "Epoch 13/30\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 1.4777 - accuracy: 0.4599\n",
      "Epoch 14/30\n",
      "250/250 [==============================] - 2s 6ms/step - loss: 1.4280 - accuracy: 0.4786\n",
      "Epoch 15/30\n",
      "250/250 [==============================] - 2s 7ms/step - loss: 1.4016 - accuracy: 0.4906\n",
      "Epoch 16/30\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 1.3846 - accuracy: 0.4995\n",
      "Epoch 17/30\n",
      "250/250 [==============================] - 2s 7ms/step - loss: 1.3515 - accuracy: 0.5141\n",
      "Epoch 18/30\n",
      "250/250 [==============================] - 1s 4ms/step - loss: 1.3411 - accuracy: 0.5145\n",
      "Epoch 19/30\n",
      "250/250 [==============================] - 2s 6ms/step - loss: 1.3026 - accuracy: 0.5232\n",
      "Epoch 20/30\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 1.2767 - accuracy: 0.5337\n",
      "Epoch 21/30\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 1.2616 - accuracy: 0.5399\n",
      "Epoch 22/30\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 1.2459 - accuracy: 0.5470\n",
      "Epoch 23/30\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 1.2078 - accuracy: 0.5616\n",
      "Epoch 24/30\n",
      "250/250 [==============================] - 2s 6ms/step - loss: 1.1745 - accuracy: 0.5710\n",
      "Epoch 25/30\n",
      "250/250 [==============================] - 2s 7ms/step - loss: 1.1578 - accuracy: 0.5817\n",
      "Epoch 26/30\n",
      "250/250 [==============================] - 2s 6ms/step - loss: 1.1412 - accuracy: 0.5851\n",
      "Epoch 27/30\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 1.1084 - accuracy: 0.5969\n",
      "Epoch 28/30\n",
      "250/250 [==============================] - 2s 6ms/step - loss: 1.0910 - accuracy: 0.6003\n",
      "Epoch 29/30\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 1.0816 - accuracy: 0.6084\n",
      "Epoch 30/30\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 1.0558 - accuracy: 0.6143\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2218b132c10>"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer = \"adam\",loss = \"sparse_categorical_crossentropy\", metrics = [\"accuracy\"])\n",
    "model.fit(X_train_v2,y_train_v2, epochs = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 0s 3ms/step - loss: 1.8911 - accuracy: 0.4070\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(X_test_v2,y_test_v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model with 30 epochs, has a accuracy of 0.7412, however when testing, the accuracy drops to 0.401\n",
    "I think this means overfitting, My model seems to work properly this time, however it is not very reliable. I need to try and optimize it "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attempt 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this attempt, I will wrangle the data before feeding it to the model, using a modified show_image function, and include a extra flatten layer in my model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_v3 = data[:8000]\n",
    "X_test_v3 = data[8000:10000]\n",
    "\n",
    "y_train_v3 = labels[:8000]\n",
    "y_test_v3 = labels[8000:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_dataset(dataset):\n",
    "        \"\"\"\n",
    "        input dataset\n",
    "        converts the vector tensors inside the dataset into a eligible format\n",
    "\n",
    "        returns a dataset with converted images.\n",
    "        \"\"\"\n",
    "        res = [] \n",
    "        for i in range(len(dataset)):\n",
    "                res.append(np.transpose(dataset[i].reshape(3,32,32), axes = [1,2,0]))\n",
    "                \n",
    "        return np.array(res)/255.0 # I had to convert the list into numpy array and divide all the elements by 255.0\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_v3 = convert_dataset(X_train_v3)\n",
    "X_test_v3 = convert_dataset(X_test_v3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X_train_v3[3])\n",
    "plt.title(keys[y_train_v3[3]].decode(\"utf-8\"))\n",
    "plt.axis('off') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My images seem to be legible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the model\n",
    "\n",
    "This model will be similar to the second attempt, with a extra flatten layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(128, activation = 'relu'),\n",
    "    keras.layers.Dense(128, activation = 'relu'),\n",
    "    keras.layers.Dense(10, activation = 'softmax') # Since there are 10 output classifications\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = \"adam\",loss = \"sparse_categorical_crossentropy\", metrics = [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 0.7520 - accuracy: 0.7343\n",
      "Epoch 2/30\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 0.7586 - accuracy: 0.7299\n",
      "Epoch 3/30\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 0.7705 - accuracy: 0.7206\n",
      "Epoch 4/30\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 0.7545 - accuracy: 0.7268\n",
      "Epoch 5/30\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 0.7501 - accuracy: 0.7303\n",
      "Epoch 6/30\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 0.7438 - accuracy: 0.7289\n",
      "Epoch 7/30\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 0.7165 - accuracy: 0.7429\n",
      "Epoch 8/30\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 0.7282 - accuracy: 0.7344\n",
      "Epoch 9/30\n",
      "250/250 [==============================] - 2s 6ms/step - loss: 0.7112 - accuracy: 0.7400\n",
      "Epoch 10/30\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 0.6871 - accuracy: 0.7535\n",
      "Epoch 11/30\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 0.7108 - accuracy: 0.7390\n",
      "Epoch 12/30\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 0.6845 - accuracy: 0.7521\n",
      "Epoch 13/30\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 0.6763 - accuracy: 0.7566\n",
      "Epoch 14/30\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 0.6691 - accuracy: 0.7558\n",
      "Epoch 15/30\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 0.6748 - accuracy: 0.7588\n",
      "Epoch 16/30\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 0.6851 - accuracy: 0.7484\n",
      "Epoch 17/30\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 0.6385 - accuracy: 0.7661\n",
      "Epoch 18/30\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 0.6590 - accuracy: 0.7651\n",
      "Epoch 19/30\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 0.6269 - accuracy: 0.7769\n",
      "Epoch 20/30\n",
      "250/250 [==============================] - 2s 6ms/step - loss: 0.6318 - accuracy: 0.7696\n",
      "Epoch 21/30\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 0.6209 - accuracy: 0.7739\n",
      "Epoch 22/30\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 0.6211 - accuracy: 0.7728\n",
      "Epoch 23/30\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 0.6251 - accuracy: 0.7735\n",
      "Epoch 24/30\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 0.6291 - accuracy: 0.7765\n",
      "Epoch 25/30\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 0.5889 - accuracy: 0.7866\n",
      "Epoch 26/30\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 0.5862 - accuracy: 0.7879\n",
      "Epoch 27/30\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 0.5908 - accuracy: 0.7789\n",
      "Epoch 28/30\n",
      "250/250 [==============================] - 1s 5ms/step - loss: 0.5804 - accuracy: 0.7937\n",
      "Epoch 29/30\n",
      "250/250 [==============================] - 1s 6ms/step - loss: 0.5760 - accuracy: 0.7911\n",
      "Epoch 30/30\n",
      "250/250 [==============================] - 2s 6ms/step - loss: 0.5715 - accuracy: 0.7951\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x27b047c2580>"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_v3,y_train_v3,batch_size= 32,epochs = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 0s 4ms/step - loss: 2.9196 - accuracy: 0.4070\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(X_test_v3,y_test_v3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feeding in (3,32,32) tensors as opposed to (3072,) seems to have no effect on the performance of the model. when evaluating the model. It still has the exact same accuracy of 0.407, and is overfitted with a accuracy of 0.7951"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attempt 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "71f4cc5eb07bc0d3ebcedfacc2ab6af50243ce67f8422c9aca860a9f159ae015"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
